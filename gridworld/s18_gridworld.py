# -*- coding: utf-8 -*-
"""01 Q-Learning - GridWorld.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SsOkpfcW7jRzRvLihBf30oLsyczCnCYL

# Assignment 6 - Algorítmo de Q-Learning

---

Universidad de los Andes - Maestría MATI <br>
**Materia:** Reinforcement Learning <br>
**Alumno:** Edgar Alexander Jayo Pacheco <br>

## Parte 01 - GridWorld
"""

import numpy as np

class gridWorldGame:
  def __init__(self, width, height, start, rewards, step_cost = -0.1):
    self.width = width
    self.height = height
    self.start = start
    self.i = start[0]
    self.j = start[1]
    self.rewards = rewards
    self.rewards_ini = rewards.copy()
    self.step_cost = step_cost

  def set(self, rewards, actions):
    self.rewards = rewards
    self.actions = actions

  def set_state(self, s):
    self.i = s[0]
    self.j = s[1]

  def current_state(self):
    return (self.i, self.j)

  def is_terminal(self, s):
    return s not in self.actions

  def move(self, action):
    if action in self.actions[(self.i, self.j)]:
      if action == 'U':
        self.i -= 1
      elif action == 'D':
        self.i += 1
      elif action == 'R':
        self.j += 1
      elif action == 'L':
        self.j -= 1
    return self.rewards.get((self.i, self.j), 0)

  def undo_move(self, action):
    if action == 'U':
      self.i += 1
    elif action == 'D':
      self.i -= 1
    elif action == 'R':
      self.j -= 1
    elif action == 'L':
      self.j += 1
    assert(self.current_state() in self.all_states())

  def game_over(self):
    return (self.i, self.j) not in self.actions

  def all_states(self):
    return set(self.actions.keys()) | set(self.rewards.keys())

  def standard_grid(self):
    # Definir el ambiente, la celda que tendrá recompensa
    # no debe tener acciones 
    g = gridWorldGame(self.height, self.width, self.start, self.rewards, self.step_cost)
    rewards = self.rewards
    actions = {
      (0, 0): ('D', 'R'),
      (0, 1): ('L', 'R'),
      (0, 2): ('L', 'D', 'R'),
      (1, 0): ('U', 'D'),
      (1, 2): ('U', 'D', 'R'),
      (2, 0): ('U', 'R'),
      (2, 1): ('L', 'R'),
      (2, 2): ('L', 'R', 'U'),
      (2, 3): ('L', 'U'),
    }
    g.set(rewards, actions)
    return g

  def negative_grid(self):
    g = self.standard_grid()
    g.rewards.update({
      (0, 0): self.step_cost,
      (0, 1): self.step_cost,
      (0, 2): self.step_cost,
      (1, 0): self.step_cost,
      (1, 2): self.step_cost,
      (2, 0): self.step_cost,
      (2, 1): self.step_cost,
      (2, 2): self.step_cost,
      (2, 3): self.step_cost,
    })
    return g

  def print_values(self, V, g):
    for i in range(g.width):
      for j in range(g.height): 
        #Evaluar si el valor está en rewards
        if (i,j) in self.rewards_ini:
          v = self.rewards_ini.get((i,j), 0)
        else:
          v = V.get((i,j), 0)
        
        if v >= 0:
          print(" %.2f|" % v, end="")
        else:
          print("%.2f|" % v, end="") 
      print("")

  def print_policy(self, P, g):
    for i in range(g.width):
      for j in range(g.height):
        a = P.get((i,j), ' ')
        print("  %s  |" % a, end="")
      print("")

  def print_qtable(self, Q):
      Q_s = dict(sorted(Q.items()))
      txt = "\t\t UP \t\t DOWN \t\t LEFT \t\t RIGHT"
      print(txt)
      txt = ""
      for i in Q_s:
        txt += str(i)
        for j in Q_s[i]:
          txt += "\t\t" + str(round(Q_s[i][j],4))
        print(txt)
        txt = ""

"""## Algoritmo Q-learning"""

def max_dict(d):
  # Retorna el ARGMAX (key) y MAX (value) de un diccionario
  max_key = None
  max_val = float('-inf')
  for k, v in d.items():
    if v > max_val:
      max_val = v
      max_key = k
  return max_key, max_val

def random_action(a, eps=0.1):
  # epsilon-soft to ensure all states are visited
  p = np.random.random()
  if p < (1 - eps):
    return a
  else:
    return np.random.choice(ALL_POSSIBLE_ACTIONS)

def QLearning(grid, start_state, ALL_POSSIBLE_ACTIONS, ALPHA, GAMMA):
  
  #################################
  # Iniciar Q(s,a)
  Q = {}
  states = grid.all_states()
  for s in states:
    Q[s] = {}
    for a in ALL_POSSIBLE_ACTIONS:
      Q[s][a] = 0

  # Valores Q iniciales para todos los estados en la cuadrícula
  #print(Q)

  update_counts = {}
  update_counts_sa = {}

  for s in states:
    update_counts_sa[s] = {}
    for a in ALL_POSSIBLE_ACTIONS:
      update_counts_sa[s][a] = 1.0
  
  #################################
  # Repetir hasta que converja
  t = 1.0
  deltas = []

  for it in range(10000):
    if it % 100 == 0:
      t += 1e-2
    #if it % 2000 == 0:
    #  print("Iteración:", it)

    s = start_state
    grid.set_state(s)

    a, _ = max_dict(Q[s])
    biggest_change = 0

    while not grid.game_over():
      a = random_action(a, eps=0.5/t) # epsilon-greedy
      # También se podría usar una acción aleatoria, pero será más lenta
      # porque puede chocar contra las paredes
      # a = np.random.choice(ALL_POSSIBLE_ACTIONS)
      r = grid.move(a)
      s2 = grid.current_state()

      # adaptive learning rate
      alpha = ALPHA / update_counts_sa[s][a]
      update_counts_sa[s][a] += 0.005

      # Actualizar Q(s,a) como la experiencia del eposodio
      old_qsa = Q[s][a]
      a2, max_q_s2a2 = max_dict(Q[s2])    
      Q[s][a] = (1 - alpha)*Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 )
      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))

      # También guardamos con qué frecuencia se ha actualizado Q(s)
      update_counts[s] = update_counts.get(s,0) + 1

      # El siguiente estado se convierte en el estado actual
      s = s2
      a = a2
    
    deltas.append(biggest_change)

  #################################
  # Determinando la política Q*
  # Encontrar V* de Q*
  policy = {}
  V = {}

  for s in grid.actions.keys():
    a, max_q = max_dict(Q[s])
    policy[s] = a
    V[s] = max_q

  #print("La frecuencia con que se ha actualizado:")
  total = np.sum(list(update_counts.values()))
  for k, v in update_counts.items():
    update_counts[k] = float(v) / total
  #gw.print_values(update_counts, grid)

  ##
  return V, policy, Q

"""# Ejecutar algoritmo

<h2> Crea instancia al Ambiente </h2>
"""

# Declaramos valores
GAMMA = 0.9
ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')
ALPHA = 0.1

# Posición donde debe iniciar
start_state = (2, 0)

# Posición donde debe finalizar y su recompensa
rewards = {(0, 3): 1, (1, 3): -1}

# Columnas y Filas del ambiente
cols = 4
rows = 3

gw = gridWorldGame(cols, rows, start_state, rewards)

grid = gw.negative_grid()

#print("Recompensas con penalidad:\n")
#gw.print_values(grid.rewards, grid)

"""<h2> Ejeuctar Q-Learning </h2>"""

ql_V, ql_policy, ql_Q = QLearning(grid, start_state, ALL_POSSIBLE_ACTIONS, ALPHA, GAMMA)

print("Valores finales:")
gw.print_values(ql_V, grid)

print("\nPolítica:")
gw.print_policy(ql_policy, grid)

print("\nQ Tabla:")
gw.print_qtable(ql_Q)

