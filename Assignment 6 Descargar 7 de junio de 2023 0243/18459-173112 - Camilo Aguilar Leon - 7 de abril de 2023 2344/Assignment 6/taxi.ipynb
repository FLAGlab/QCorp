{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17334882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_values(board, decimals = 2):\n",
    "    for i in range(0, len(board)):\n",
    "        print('-------------------------------------------------------')\n",
    "        out = '| '\n",
    "        for j in range(0, len(board[0])):\n",
    "            value = 0\n",
    "            if type(board[i][j]) == str:\n",
    "                value = board[i][j]\n",
    "            else:\n",
    "                value = round(board[i][j],decimals)\n",
    "            out += str(value).ljust(7) + ' | '\n",
    "        print(out)\n",
    "    print('-------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b928c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "class Gridworld:\n",
    "    #Dimensions[rows,cols]\n",
    "    #Each cell element [row,col,value]\n",
    "    def __init__(self, dimensions=[10,10], stop_states = [(0,0),(4,0),(0,4)], passenger_state = (4,3), cells={}, initial_state=None, actions=['up','down','left','right','pick','drop']):\n",
    "        self.dimensions = dimensions\n",
    "        self.actions = actions\n",
    "        self.cells = cells\n",
    "        self.state_actions = self.define_actions()\n",
    "        self.init_state(initial_state)\n",
    "        self.stop_states = stop_states\n",
    "        self.passenger_state = passenger_state\n",
    "        \n",
    "    def define_actions(self):\n",
    "        state_actions = {}\n",
    "        for row in range(self.dimensions[0]):\n",
    "            for col in range(self.dimensions[1]):\n",
    "                for taxi_state in ['full', 'empty']:\n",
    "                    state = (row, col, taxi_state)\n",
    "                    cell = (row,col)\n",
    "                    if cell in self.cells:\n",
    "                        actions = self.actions.copy()\n",
    "                        if 'l' in self.cells[cell]:\n",
    "                            actions.remove('left')\n",
    "                        if 't' in self.cells[cell]:\n",
    "                            actions.remove('up')\n",
    "                        if 'r' in self.cells[cell]:\n",
    "                            actions.remove('right')\n",
    "                        if 'b' in self.cells[cell]:\n",
    "                            actions.remove('down')\n",
    "                        state_actions[state] = actions\n",
    "                    else:\n",
    "                        state_actions[state] = self.actions \n",
    "        return state_actions\n",
    "        \n",
    "    def get_board(self):\n",
    "        return self.board\n",
    "        \n",
    "    def get_current_state(self):\n",
    "        return self.current_state\n",
    "    \n",
    "    def get_possible_actions(self, state=(0,0)):\n",
    "        return self.state_actions[state]\n",
    "    \n",
    "    def do_action(self, action, state = None):\n",
    "        current_state = self.current_state\n",
    "        reward = 0\n",
    "        if state != None:\n",
    "            current_state = state\n",
    "        new_state = ()\n",
    "        if(action=='up'):\n",
    "            new_state=(current_state[0]-1,current_state[1], current_state[2])\n",
    "            reward = -1\n",
    "        elif(action=='down'):\n",
    "            new_state=(current_state[0]+1,current_state[1], current_state[2])\n",
    "            reward = -1\n",
    "        elif(action=='left'):\n",
    "            new_state=(current_state[0],current_state[1]-1, current_state[2])\n",
    "            reward = -1\n",
    "        elif(action=='right'):\n",
    "            new_state=(current_state[0],current_state[1]+1, current_state[2])\n",
    "            reward = -1\n",
    "        elif(action=='drop'):\n",
    "            cell = (current_state[0], current_state[1])\n",
    "            if cell in self.stop_states and current_state[2] == 'full':\n",
    "                new_state=(current_state[0],current_state[1], 'empty')\n",
    "                reward = 10\n",
    "            else:\n",
    "                reward = -10\n",
    "                new_state=(current_state[0],current_state[1], current_state[2])\n",
    "        elif(action=='pick'):\n",
    "            cell = (current_state[0], current_state[1])\n",
    "            if cell == self.passenger_state and current_state[2] == 'empty':\n",
    "                new_state=(current_state[0],current_state[1], 'full')\n",
    "                reward = 5\n",
    "            else:\n",
    "                reward = -10\n",
    "                new_state=(current_state[0],current_state[1], current_state[2])   \n",
    "        \n",
    "        return [reward, new_state]\n",
    "    \n",
    "    def init_state(self, initial_state=None):\n",
    "        if initial_state:\n",
    "            self.initial_state = initial_state\n",
    "            self.current_state = initial_state\n",
    "        else:\n",
    "            new_initial_state = random.choice(list(self.state_actions.keys()))\n",
    "            self.initial_state = new_initial_state\n",
    "            self.current_state = new_initial_state\n",
    "        \n",
    "    def is_terminal(self, action, state):\n",
    "        cell = (state[0], state[1])\n",
    "        if action == 'drop' and cell in self.stop_states and state[2] == 'full':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38c6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    #Dimensions[rows,cols]\n",
    "    #Each cell element [row,col,value]\n",
    "    def __init__(self, mdp, discount=0.9, alpha=0.5, iterations=False, epsilon=0.9):\n",
    "        # Mdp is equivalent to env\n",
    "        self.mdp = mdp\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.iterations = iterations\n",
    "        self.epsilon = epsilon\n",
    "        self.q = {}\n",
    "        state_actions = self.mdp.state_actions\n",
    "        for state in state_actions.keys():\n",
    "            for action in state_actions[state]:\n",
    "                self.q[(state[0],state[1],state[2],action)] = 0\n",
    "\n",
    "    def run_episode(self):\n",
    "        is_terminal = False\n",
    "        while not is_terminal:\n",
    "            state1 = self.mdp.current_state\n",
    "            action = self.choose_action(state1)\n",
    "            action1 = action[0]\n",
    "            is_terminal = self.mdp.is_terminal(action1, state1)\n",
    "            res_do_action = self.mdp.do_action(action[0], self.mdp.current_state)\n",
    "            state2 = res_do_action[1]\n",
    "            action2 = self.choose_best_action(state2)[0]\n",
    "            reward = res_do_action[0]\n",
    "            self.action_function(state1,action1,reward,state2,action2)\n",
    "            self.mdp.current_state = state2\n",
    "    \n",
    "    def run_value_iteration(self):\n",
    "        # Begins at iteration 2 because first iteration is initializing rewards\n",
    "        converge = 0\n",
    "        i = 1\n",
    "        while self.iterations >= i:\n",
    "            i += 1\n",
    "            self.mdp.init_state()\n",
    "            self.run_episode()\n",
    "        print(\"Total iterations: \" + str(i))\n",
    "    \n",
    "    def action_function(self,state1,action1,reward,state2,action2):\n",
    "        self.q[(state1[0], state1[1], state1[2], action1)] = (1-self.alpha)*self.q[(state1[0], state1[1], state1[2], action1)] + self.alpha*(reward + self.discount*self.q[(state2[0], state2[1], state1[2], action2)])\n",
    "    \n",
    "    def choose_best_action(self, state):\n",
    "        possible_actions = self.mdp.get_possible_actions(state)\n",
    "        best_actions = []\n",
    "        best_q_value = -9999999\n",
    "        for action in possible_actions:\n",
    "            if len(best_actions) == 0:\n",
    "                best_actions.append(action)\n",
    "                best_q_value = self.q[(state[0], state[1], state[2], action)]\n",
    "            else:\n",
    "                if best_q_value == self.q[(state[0], state[1], state[2], action)]:\n",
    "                    best_actions.append(action)\n",
    "                elif best_q_value < self.q[(state[0], state[1], state[2], action)]:\n",
    "                    best_actions = [action]\n",
    "                    best_q_value = self.q[(state[0], state[1], state[2], action)]\n",
    "                    \n",
    "        best_action = random.choice(best_actions)\n",
    "        return [best_action, best_action]\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        possible_actions = self.mdp.get_possible_actions(state).copy()\n",
    "        best_actions = []\n",
    "        best_q_value = -9999999\n",
    "        for action in possible_actions:\n",
    "            if len(best_actions) == 0:\n",
    "                best_actions.append(action)\n",
    "                best_q_value = self.q[(state[0], state[1], state[2], action)]\n",
    "            else:\n",
    "                if best_q_value == self.q[(state[0], state[1], state[2], action)]:\n",
    "                    best_actions.append(action)\n",
    "                elif best_q_value < self.q[(state[0], state[1], state[2], action)]:\n",
    "                    best_actions = [action]\n",
    "                    best_q_value = self.q[(state[0], state[1], state[2], action)]\n",
    "                    \n",
    "        best_action = random.choice(best_actions)\n",
    "        if random.random() < (1-self.epsilon) :\n",
    "            return [best_action, best_action]\n",
    "        else:\n",
    "            possible_actions.remove(best_action)\n",
    "            return [random.choice(possible_actions), best_action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3bb7330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 3001\n"
     ]
    }
   ],
   "source": [
    "grid = Gridworld(cells={(0, 0): 'lt', (0, 1): 'tr',(0, 2): 'lt', (0, 3): 't', (0, 4): 'tr', (1, 0): 'l', (1, 1): 'r', (1, 2): 'l', (1, 4): 'r', (2,0): 'l', (2, 4):'r', (3,0):'lr', (3,1): 'l', (3,2):'r',(3,3):'l',(3,4):'r',(4,0):'lrb',(4,1):'lb',(4,2):'rb',(4,3):'lb',(4,4):'rb'}\n",
    "                        ,dimensions = [5,5]\n",
    "                        ,stop_states = [(0,0),(4,0),(0,4)]\n",
    "                        ,passenger_state = (4,3))\n",
    "\n",
    "iteration = QLearning(grid, discount = 0.8, alpha=0.2, iterations = 3000, epsilon=0.1)\n",
    "iteration.run_value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba2cd19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| 15.14   | 10.92   | 9.43    | 13.03   | 17.54   | \n",
      "-------------------------------------------------------\n",
      "| 11.1    | 7.81    | 6.54    | 9.43    | 13.03   | \n",
      "-------------------------------------------------------\n",
      "| 7.57    | 5.12    | 4.23    | 6.54    | 9.43    | \n",
      "-------------------------------------------------------\n",
      "| 8.94    | 0.91    | 2.39    | 4.23    | 6.54    | \n",
      "-------------------------------------------------------\n",
      "| 12.5    | -0.28   | 0.91    | 2.39    | 4.23    | \n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "board = np.full((5, 5), float('-inf'))\n",
    "for (x, y, taxi, action), value in iteration.q.items():\n",
    "    if taxi == 'full':\n",
    "        board[x][y] = max(board[x][y], value)\n",
    "show_values(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c87012f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| -2.42   | -1.78   | -0.98   | 0.03    | -0.98   | \n",
      "-------------------------------------------------------\n",
      "| -1.78   | -0.98   | 0.03    | 1.29    | 0.03    | \n",
      "-------------------------------------------------------\n",
      "| -0.97   | 0.03    | 1.29    | 2.86    | 1.29    | \n",
      "-------------------------------------------------------\n",
      "| -1.78   | -0.97   | 0.03    | 4.83    | 2.86    | \n",
      "-------------------------------------------------------\n",
      "| -2.43   | -1.78   | -0.98   | 7.29    | 4.82    | \n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "board = np.full((5, 5), float('-inf'))\n",
    "for (x, y, taxi, action), value in iteration.q.items():\n",
    "    if taxi == 'empty':\n",
    "        board[x][y] = max(board[x][y], value)\n",
    "show_values(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a32623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x    y  taxi    up    down    left    right      pick    drop\n",
      "---  ---  ------  ----  ------  ------  -------  ------  ------\n",
      "  0    0  empty   -     -2.5    -       -2.5      -10      -6.5\n",
      "  0    1  empty   -     -2.0    -3.0    -          -6.5    -8\n",
      "  0    2  empty   -     -1.0    -       -1.5       -8      -6.5\n",
      "  0    3  empty   -     0.0     -1.5    -1.5       -7      -6.5\n",
      "  0    4  empty   -     -1.0    -1.5    -          -4      -5.5\n",
      "  1    0  empty   -2.5  -2.0    -       -2.0       -8.5   -10\n",
      "  1    1  empty   -2.5  -1.0    -2.0    -          -6.5    -8.5\n",
      "  1    2  empty   -1.5  -1.0    -       0.0        -7      -7\n",
      "  1    3  empty   -1.0  1.5     -1.5    -1.0       -8.5    -7\n",
      "  1    4  empty   -1.5  -0.5    0.0     -          -7      -8\n",
      "  2    0  empty   -2.5  -2.5    -       -1.0      -10      -9.5\n",
      "  2    1  empty   -2.0  -2.0    -2.0    0.0        -9.5   -10\n",
      "  2    2  empty   -1.0  -1.0    -1.0    1.5        -9      -9\n",
      "  2    3  empty   0.0   3.0     0.0     0.0        -7.5    -7.5\n",
      "  2    4  empty   -1.0  0.5     1.5     -          -6.5    -6.5\n",
      "  3    0  empty   -2.0  -3.0    -       -          -8.5    -7.5\n",
      "  3    1  empty   -1.0  -2.5    -       -1.5       -7.5    -8.5\n",
      "  3    2  empty   0.0   -2.0    -2.0    -          -7.5    -5\n",
      "  3    3  empty   1.5   5.0     -       1.5        -6      -6\n",
      "  3    4  empty   -0.5  -0.5    3.0     -          -2      -5.5\n",
      "  4    0  empty   -2.5  -       -       -          -9.5    -8.5\n",
      "  4    1  empty   -2.0  -       -       -2.5       -7.5    -7.5\n",
      "  4    2  empty   -1.0  -       -2.5    -          -9.5    -6.5\n",
      "  4    3  empty   3.0   -       -       3.0         7.5    -4\n",
      "  4    4  empty   0.5   -       5.0     -          -4      -6\n",
      "  0    0  full    -     6.5     -       6.0         0.5    15\n",
      "  0    1  full    -     2.0     11.0    -          -3      -2.5\n",
      "  0    2  full    -     3.5     -       9.5        -2.5    -3.5\n",
      "  0    3  full    -     5.5     4.0     13.0        0.5    -0.5\n",
      "  0    4  full    -     9.5     9.5     -           4      17.5\n",
      "  1    0  full    11.0  1.5     -       3.5        -2      -2\n",
      "  1    1  full    2.0   0.5     8.0     -          -4.5    -5\n",
      "  1    2  full    6.5   0.0     -       2.0        -3.5    -2.5\n",
      "  1    3  full    4.5   2.5     3.0     9.5        -2.5    -2\n",
      "  1    4  full    13.0  6.5     6.5     -           0.5     0.5\n",
      "  2    0  full    7.5   -0.5    -       0.0        -4      -3\n",
      "  2    1  full    5.0   -1.0    -1.0    -0.5       -4      -4\n",
      "  2    2  full    0.0   0.0     1.5     4.0        -6.5    -6\n",
      "  2    3  full    6.5   2.5     2.5     6.5        -5      -5\n",
      "  2    4  full    9.5   4.0     4.0     -          -2.5    -2.5\n",
      "  3    0  full    1.0   9.0     -       -          -3.5    -3\n",
      "  3    1  full    0.0   -1.5    -       1.0        -3.5    -6\n",
      "  3    2  full    2.5   -0.5    -1.0    -          -6.5    -4\n",
      "  3    3  full    4.0   1.0     -       4.0        -6.5    -6.5\n",
      "  3    4  full    6.5   2.0     1.5     -          -4      -4.5\n",
      "  4    0  full    3.0   -       -       -          -0.5    12.5\n",
      "  4    1  full    -0.5  -       -       -2.0       -3.5    -7\n",
      "  4    2  full    1.0   -       -1.5    -          -5      -6\n",
      "  4    3  full    2.5   -       -       2.5        -8      -8\n",
      "  4    4  full    4.0   -       0.0     -          -5.5    -7\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "my_dict = iteration.q\n",
    "\n",
    "# Create a dictionary to store the table data\n",
    "table_data = {}\n",
    "\n",
    "# Iterate through the keys in the dictionary\n",
    "for key in my_dict.keys():\n",
    "    # Extract x, y, and action from the key\n",
    "    x, y, taxi_state, action = key\n",
    "\n",
    "    # If x, y pair is not in table_data, create a new row with x, y as the key\n",
    "    if (x, y, taxi_state) not in table_data:\n",
    "        table_data[(x, y, taxi_state)] = {}\n",
    "\n",
    "    # Set the value of the action column in the table data\n",
    "    table_data[(x, y, taxi_state)][action] = round(my_dict[key]*2)/2\n",
    "\n",
    "# Convert the dictionary to a list of rows\n",
    "table_rows = []\n",
    "for key, value in table_data.items():\n",
    "    if key[2] == 'empty':\n",
    "        row = list(key) + [value.get('up', '-'), value.get('down', '-'), value.get('left', '-'), value.get('right', '-'), value.get('pick', '-'), value.get('drop', '-')]\n",
    "        table_rows.append(row)\n",
    "    \n",
    "for key, value in table_data.items():\n",
    "    if key[2] == 'full':\n",
    "        row = list(key) + [value.get('up', '-'), value.get('down', '-'), value.get('left', '-'), value.get('right', '-'), value.get('pick', '-'), value.get('drop', '-')]\n",
    "        table_rows.append(row)\n",
    "\n",
    "# Print the table using tabulate\n",
    "print(tabulate(table_rows, headers=['x', 'y', 'taxi', 'up','down','left','right','pick','drop']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ca3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
